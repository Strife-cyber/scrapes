use anyhow::{Context, Result};
use reqwest::Client;
use scraper::{Html, Selector};
use serde::{Deserialize, Serialize};
use tracing::{info, warn};
use url::Url;
use tokio::sync::Semaphore;
use std::sync::Arc;
use futures::stream::{self, StreamExt};
use webbrowser;

/// Structure repr√©sentant une saison avec ses √©pisodes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Season {
    pub name: String,
    pub url: String,
    pub episodes: Vec<Episode>,
}

/// Structure repr√©sentant un √©pisode avec ses liens de t√©l√©chargement
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Episode {
    pub name: String,
    pub download_links: Vec<DownloadLink>,
}

/// Structure repr√©sentant un lien de t√©l√©chargement
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DownloadLink {
    pub quality: String,
    pub url: String,
    pub file_id: Option<String>,
    pub dkey: Option<String>,
    pub actual_download_urls: Vec<String>,
}

/// Scraper sp√©cialis√© pour FZTV Series
pub struct FztvScraper {
    client: Client,
    base_url: String,
    // Semaphore pour limiter les requ√™tes concurrentes
    semaphore: Arc<Semaphore>,
}

impl FztvScraper {
    /// Cr√©e une nouvelle instance du scraper FZTV
    pub fn new(base_url: String) -> Self {
        let client = Client::builder()
            .user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            .timeout(std::time::Duration::from_secs(30))
            .build()
            .expect("Impossible de cr√©er le client HTTP");

        // Limite √† 10 requ√™tes concurrentes pour ne pas surcharger le serveur
        let semaphore = Arc::new(Semaphore::new(10));

        Self { client, base_url, semaphore }
    }

    /// Ouvre une URL dans le navigateur par d√©faut pour debug (ACTIV√â pour le test)
    fn open_in_browser(&self, url: &str, description: &str) {
        info!("üåê Ouverture dans le navigateur: {} - {}", description, url);
        if let Err(e) = webbrowser::open(url) {
            warn!("Impossible d'ouvrir le navigateur pour {}: {}", url, e);
        }
    }

    /// Scrape toutes les saisons disponibles sur la page principale
    pub async fn scrape_seasons(&self, main_url: &str) -> Result<Vec<Season>> {
        info!("D√©but du scraping des saisons FZTV depuis: {}", main_url);
        
        // Ouvrir la page principale dans le navigateur pour debug
        self.open_in_browser(main_url, "Page Principale FZTV");
        
        let html = self.fetch_page(main_url).await?;
        let document = Html::parse_document(&html);
        
        // S√©lecteur pour les liens de saisons avec itemprop="url"
        let season_selector = Selector::parse("a[itemprop=\"url\"]")
            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les saisons: {}", e))?;
        
        // Collecter toutes les infos de saisons d'abord
        let mut season_infos = Vec::new();
        
        for element in document.select(&season_selector) {
            if let Some(href) = element.value().attr("href") {
                let name_selector = Selector::parse("span[itemprop=\"name\"]")
                    .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour le nom de saison: {}", e))?;
                
                let season_name = element
                    .select(&name_selector)
                    .next()
                    .and_then(|span| span.text().next())
                    .unwrap_or("Saison inconnue")
                    .to_string();
                
                // Construire l'URL compl√®te de la saison
                let season_url = self.resolve_url(href)?;
                
                info!("Saison trouv√©e: {} -> {}", season_name, season_url);
                season_infos.push((season_name, season_url));
            }
        }
        
        // Scraper toutes les saisons en parall√®le avec contr√¥le de concurrence
        let seasons = stream::iter(season_infos)
            .map(|(name, url)| async move {
                let episodes = self.scrape_episodes(&url).await.ok()?;
                Some(Season {
                    name,
                    url,
                    episodes,
                })
            })
            .buffer_unordered(10)  // Traiter jusqu'√† 10 saisons en parall√®le
            .filter_map(|x| async { x })
            .collect::<Vec<_>>()
            .await;
        
        info!("{} saisons FZTV trouv√©es", seasons.len());
        Ok(seasons)
    }

    /// Scrape tous les √©pisodes d'une saison donn√©e
    /// Scrape les √©pisodes d'une saison sp√©cifique
    pub async fn scrape_episodes(&self, season_url: &str) -> Result<Vec<Episode>> {
        info!("Scraping des √©pisodes FZTV pour: {}", season_url);
        
        // Ouvrir la page de saison dans le navigateur pour debug
        self.open_in_browser(season_url, "Page Saison");
        
        let html = self.fetch_page(season_url).await?;
        let document = Html::parse_document(&html);
        
        // Debug: Afficher une partie du HTML pour comprendre la structure
        self.debug_html_structure(&document, season_url).await?;
        
        // Essayer diff√©rents s√©lecteurs pour trouver les √©pisodes
        let mut episodes = Vec::new();
        
        // S√©lecteur 1: ul.list (original)
        if let Ok(selector) = Selector::parse("ul.list") {
            episodes.extend(self.scrape_episodes_with_selector(&document, &selector, "ul.list").await?);
        }
        
        // S√©lecteur 2: div avec class contenant "episode" ou "list"
        if episodes.is_empty() {
            if let Ok(selector) = Selector::parse("div[class*=\"episode\"], div[class*=\"list\"]") {
                episodes.extend(self.scrape_episodes_with_selector(&document, &selector, "div.episode/list").await?);
            }
        }
        
        // S√©lecteur 3: table ou tr pour les √©pisodes
        if episodes.is_empty() {
            if let Ok(selector) = Selector::parse("table tr, tr") {
                episodes.extend(self.scrape_episodes_with_selector(&document, &selector, "table tr").await?);
            }
        }
        
        // S√©lecteur 4: liens avec onclick contenant "episode"
        if episodes.is_empty() {
            if let Ok(selector) = Selector::parse("a[onclick*=\"episode\"]") {
                episodes.extend(self.scrape_episodes_with_selector(&document, &selector, "a[onclick*=\"episode\"]").await?);
            }
        }
        
        info!("{} √©pisodes FZTV trouv√©s pour cette saison", episodes.len());
        Ok(episodes)
    }
    
    /// Debug function pour examiner la structure HTML
    async fn debug_html_structure(&self, document: &Html, season_url: &str) -> Result<()> {
        info!("=== DEBUG HTML STRUCTURE pour {} ===", season_url);
        
        // Chercher tous les √©l√©ments qui pourraient contenir des √©pisodes
        let debug_selectors = vec![
            "ul", "div", "table", "tr", "li",
            "a[onclick]", "a[href*=\"episode\"]", "a[href*=\"download\"]",
            "[class*=\"episode\"]", "[class*=\"list\"]", "[class*=\"download\"]"
        ];
        
        for selector_str in debug_selectors {
            if let Ok(selector) = Selector::parse(selector_str) {
                let count = document.select(&selector).count();
                if count > 0 {
                    info!("S√©lecteur '{}': {} √©l√©ments trouv√©s", selector_str, count);
                    
                    // Afficher les premiers √©l√©ments pour comprendre la structure
                    for (i, element) in document.select(&selector).enumerate() {
                        if i >= 3 { break; } // Limiter √† 3 exemples
                        let text = element.text().collect::<String>().trim().to_string();
                        let text_preview = if text.len() > 100 { 
                            format!("{}...", &text[..100]) 
                        } else { 
                            text 
                        };
                        info!("  Exemple {}: {}", i + 1, text_preview);
                        
                        // Afficher les attributs importants
                        if let Some(onclick) = element.value().attr("onclick") {
                            info!("    onclick: {}", onclick);
                        }
                        if let Some(href) = element.value().attr("href") {
                            info!("    href: {}", href);
                        }
                        if let Some(class) = element.value().attr("class") {
                            info!("    class: {}", class);
                        }
                    }
                }
            }
        }
        
        info!("=== FIN DEBUG HTML STRUCTURE ===");
        Ok(())
    }
    
    /// Scrape les √©pisodes avec un s√©lecteur sp√©cifique
    async fn scrape_episodes_with_selector(&self, document: &Html, selector: &Selector, selector_name: &str) -> Result<Vec<Episode>> {
        let mut episodes = Vec::new();
        
        info!("Tentative de scraping avec le s√©lecteur: {}", selector_name);
        
        for (episode_index, element) in document.select(selector).enumerate() {
            let mut download_links = Vec::new();
            
            // Essayer d'extraire le nom de l'√©pisode
            let episode_name = self.extract_episode_name_from_element(&element, episode_index);
            
            // Chercher les liens de t√©l√©chargement dans cet √©l√©ment
            let link_selector = Selector::parse("a[onclick*=\"window.open\"], a[onclick*=\"episode\"], a[href*=\"download\"]")
                .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les liens: {}", e))?;
            
            for link_element in element.select(&link_selector) {
                if let Some(onclick) = link_element.value().attr("onclick") {
                    // Extraire l'URL de t√©l√©chargement, le fileid et le dkey
                    if let Some((download_url, file_id, dkey)) = self.parse_onclick(onclick) {
                        let quality_selector = Selector::parse("small, span, b")
                            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour la qualit√©: {}", e))?;
                        
                        let quality = link_element
                            .select(&quality_selector)
                            .next()
                            .and_then(|elem| elem.text().next())
                            .unwrap_or("Qualit√© inconnue")
                            .to_string();
                        
                        let download_link = DownloadLink {
                            quality,
                            url: download_url,
                            file_id: Some(file_id),
                            dkey,
                            actual_download_urls: Vec::new(),
                        };
                        
                        download_links.push(download_link);
                    }
                }
                
                // Essayer aussi avec href direct
                if let Some(href) = link_element.value().attr("href") {
                    if href.contains("episode") || href.contains("download") {
                        let quality = "Direct Link".to_string();
                        let download_link = DownloadLink {
                            quality,
                            url: href.to_string(),
                            file_id: None,
                            dkey: None,
                            actual_download_urls: Vec::new(),
                        };
                        download_links.push(download_link);
                    }
                }
            }
            
            if !download_links.is_empty() {
                episodes.push(Episode {
                    name: episode_name,
                    download_links,
                });
            }
        }
        
        info!("S√©lecteur '{}': {} √©pisodes trouv√©s", selector_name, episodes.len());
        Ok(episodes)
    }
    
    /// Extrait le nom de l'√©pisode depuis un √©l√©ment
    fn extract_episode_name_from_element(&self, element: &scraper::ElementRef, episode_index: usize) -> String {
        // Essayer de trouver du texte dans l'√©l√©ment ou ses enfants
        let text = element.text().collect::<String>().trim().to_string();
        
        if !text.is_empty() && text.len() > 3 {
            // Prendre les premiers mots comme nom d'√©pisode
            let words: Vec<&str> = text.split_whitespace().take(5).collect();
            if !words.is_empty() {
                return words.join(" ");
            }
        }
        
        // Fallback: chercher dans les √©l√©ments enfants
        let name_selectors = vec!["b", "strong", "span", "div", "a"];
        for selector_str in name_selectors {
            if let Ok(selector) = Selector::parse(selector_str) {
                if let Some(name_elem) = element.select(&selector).next() {
                    let name_text = name_elem.text().collect::<String>().trim().to_string();
                    if !name_text.is_empty() && name_text.len() > 3 {
                        return name_text;
                    }
                }
            }
        }
        
        // Dernier recours: nom g√©n√©rique
        format!("√âpisode {}", episode_index + 1)
    }

    /// Parse le contenu onclick pour extraire l'URL de window.location.href, le fileid et le dkey
    fn parse_onclick(&self, onclick: &str) -> Option<(String, String, Option<String>)> {
        // Rechercher l'URL dans window.location.href (c'est l'URL importante, pas window.open)
        if let Some(start) = onclick.find("window.location.href=\"") {
            let start = start + 22; // Longueur de "window.location.href=\""
            if let Some(end) = onclick[start..].find("\"") {
                let url = &onclick[start..start + end];
                
                // Rechercher le fileid dans l'URL
                if let Some(fileid_start) = onclick.find("fileid=") {
                    let fileid_start = fileid_start + 7; // Longueur de "fileid="
                    if let Some(fileid_end) = onclick[fileid_start..].find("&") {
                        let file_id = &onclick[fileid_start..fileid_start + fileid_end];
                        
                        // Rechercher le dkey
                        let dkey = if let Some(dkey_start) = onclick.find("dkey=") {
                            let dkey_start = dkey_start + 5; // Longueur de "dkey="
                            if let Some(dkey_end) = onclick[dkey_start..].find("\"") {
                                Some(onclick[dkey_start..dkey_start + dkey_end].to_string())
                            } else {
                                None
                            }
                        } else {
                            None
                        };
                        
                        return Some((url.to_string(), file_id.to_string(), dkey));
                    }
                }
            }
        }
        None
    }

    /// Scrape les URLs de t√©l√©chargement r√©elles avec traitement rapide pour √©viter l'expiration
    async fn scrape_download_urls_fast(&self, download_page_url: &str) -> Result<Vec<String>> {
        info!("üöÄ Scraping rapide des URLs de t√©l√©chargement depuis: {}", download_page_url);
        
        // Construire l'URL compl√®te si n√©cessaire
        let full_url = if download_page_url.starts_with("http") {
            download_page_url.to_string()
        } else {
            self.resolve_url(download_page_url)?
        };
        
        info!("URL compl√®te pour le scraping rapide: {}", full_url);
        
        // Ouvrir la page de t√©l√©chargement dans le navigateur pour debug
        self.open_in_browser(&full_url, "Page de T√©l√©chargement");
        
        let html = match self.fetch_page(&full_url).await {
            Ok(html) => html,
            Err(e) => {
                warn!("Erreur lors de la r√©cup√©ration de la page {}: {}", full_url, e);
                return Ok(Vec::new()); // Retourner une liste vide au lieu d'√©chouer
            }
        };
        
        let document = Html::parse_document(&html);
        
        // Si c'est une page episode.php, chercher le lien "DOWNLOAD THIS EPISODE ON YOUR DEVICE"
        if download_page_url.contains("episode.php") {
            return self.scrape_episode_page(&document).await;
        }
        
        // Si c'est une page downloadmp4.php, chercher directement les liens
        if download_page_url.contains("downloadmp4.php") {
            return self.scrape_download_page_fast(&document).await;
        }
        
        // Sinon, essayer de scraper directement
        self.scrape_download_page_fast(&document).await
    }

    /// Scrape les URLs de t√©l√©chargement r√©elles depuis la page de t√©l√©chargement
    async fn scrape_download_urls(&self, download_page_url: &str) -> Result<Vec<String>> {
        info!("Scraping des URLs de t√©l√©chargement FZTV depuis: {}", download_page_url);
        
        // Construire l'URL compl√®te si n√©cessaire
        let full_url = if download_page_url.starts_with("http") {
            download_page_url.to_string()
        } else {
            self.resolve_url(download_page_url)?
        };
        
        info!("URL compl√®te pour le scraping: {}", full_url);
        
        // Ouvrir la page de t√©l√©chargement dans le navigateur pour debug
        self.open_in_browser(&full_url, "Page Download Final");
        
        let html = match self.fetch_page(&full_url).await {
            Ok(html) => html,
            Err(e) => {
                info!("Erreur lors de la r√©cup√©ration de la page {}: {}", full_url, e);
                return Ok(Vec::new()); // Retourner une liste vide au lieu d'√©chouer
            }
        };
        
        let document = Html::parse_document(&html);
        
        // Si c'est une page episode.php, chercher le lien "DOWNLOAD THIS EPISODE ON YOUR DEVICE"
        if download_page_url.contains("episode.php") {
            return self.scrape_episode_page(&document).await;
        }
        
        // Si c'est une page downloadmp4.php, chercher directement les liens
        if download_page_url.contains("downloadmp4.php") {
            return self.scrape_download_page(&document).await;
        }
        
        // Sinon, essayer de scraper directement
        self.scrape_download_page(&document).await
    }

    /// Scrape une page episode.php pour trouver le lien de t√©l√©chargement
    async fn scrape_episode_page(&self, document: &Html) -> Result<Vec<String>> {
        info!("Recherche du lien dlink2 dans la page episode.php FZTV");
        
        // Chercher le lien "DOWNLOAD THIS EPISODE ON YOUR DEVICE"
        let download_link_selector = Selector::parse("a[id=\"dlink2\"]")
            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour le lien de t√©l√©chargement: {}", e))?;
        
        let mut found_links = 0;
        for element in document.select(&download_link_selector) {
            found_links += 1;
            if let Some(href) = element.value().attr("href") {
                // Construire l'URL compl√®te
                let full_download_url = self.resolve_url(href)?;
                info!("Lien de t√©l√©chargement FZTV trouv√©: {}", full_download_url);
                
                // Naviguer vers cette page et scraper les URLs r√©elles
                return self.scrape_download_page_from_url(&full_download_url).await;
            }
        }
        
        info!("Aucun lien dlink2 FZTV trouv√© ({} √©l√©ments trouv√©s)", found_links);
        Ok(Vec::new())
    }

    /// Scrape une page download.php pour extraire les URLs de t√©l√©chargement (version rapide)
    async fn scrape_download_page_fast(&self, document: &Html) -> Result<Vec<String>> {
        info!("üöÄ Recherche rapide des URLs de t√©l√©chargement r√©elles dans la page");
        
        let mut download_urls = Vec::new();
        
        // M√©thode 1: Chercher les textbox avec les URLs directes (PRIORIT√â ABSOLUE - bas√© sur l'observation du navigateur)
        let textbox_selector = Selector::parse("textbox")
            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les textbox: {}", e))?;
        
        for textbox in document.select(&textbox_selector) {
            if let Some(value) = textbox.value().attr("value") {
                if value.starts_with("http") && !value.contains("t.me") && !value.contains("instagram") && !value.contains("fzmovies.live") {
                    download_urls.push(value.to_string());
                    info!("üéØ URL de t√©l√©chargement r√©elle trouv√©e (textbox): {}", value);
                }
            }
        }
        
        // M√©thode 2: Chercher dans div.downloadlinks2 avec input[name="filelink"] (fallback)
        if download_urls.is_empty() {
            let download_links_selector = Selector::parse("div.downloadlinks2")
                .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les liens de t√©l√©chargement: {}", e))?;
            
            for element in document.select(&download_links_selector) {
            info!("‚úÖ Div downloadlinks2 trouv√©, recherche des inputs filelink");
            
            // Chercher les inputs avec name="filelink"
            let input_selector = Selector::parse("input[name=\"filelink\"]")
                .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les inputs: {}", e))?;
            
            for input in element.select(&input_selector) {
                if let Some(value) = input.value().attr("value") {
                    // V√©rifier que c'est une URL de t√©l√©chargement valide (pas de liens sociaux)
                    if value.starts_with("http") && !value.contains("t.me") && !value.contains("instagram") && !value.contains("fzmovies.live") {
                        download_urls.push(value.to_string());
                        info!("üéØ URL de t√©l√©chargement r√©elle trouv√©e dans downloadlinks2: {}", value);
                    } else {
                        info!("‚ö†Ô∏è URL ignor√©e (lien social ou invalide): {}", value);
                    }
                }
            }
            }
        }
        
        // M√©thode 3: Si pas trouv√©, chercher directement tous les inputs filelink
        if download_urls.is_empty() {
            info!("‚ö†Ô∏è Aucun div.downloadlinks2 trouv√©, recherche directe des inputs filelink");
            let input_selector = Selector::parse("input[name=\"filelink\"]")
                .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les inputs: {}", e))?;
            
            for input in document.select(&input_selector) {
                if let Some(value) = input.value().attr("value") {
                    // V√©rifier que c'est une URL de t√©l√©chargement valide
                    if value.starts_with("http") && !value.contains("t.me") && !value.contains("instagram") && !value.contains("fzmovies.live") {
                        download_urls.push(value.to_string());
                        info!("üéØ URL de t√©l√©chargement r√©elle trouv√©e (directe): {}", value);
                    } else {
                        info!("‚ö†Ô∏è URL ignor√©e (lien social ou invalide): {}", value);
                    }
                }
            }
        }
        
        // M√©thode 4: Chercher les liens flink1, flink2, etc.
        if download_urls.is_empty() {
            info!("‚ö†Ô∏è Aucun input filelink trouv√©, recherche des liens flink");
            let flink_selector = Selector::parse("a[id^=\"flink\"]")
                .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour flink: {}", e))?;
            
            for link in document.select(&flink_selector) {
                if let Some(href) = link.value().attr("href") {
                    if href.starts_with("http") {
                        download_urls.push(href.to_string());
                        info!("üéØ Lien flink trouv√©: {}", href);
                    }
                }
            }
        }
        
        // M√©thode 5: Recherche de tous les √©l√©ments contenant des URLs (fallback)
        if download_urls.is_empty() {
            info!("‚ö†Ô∏è Aucun lien sp√©cifique trouv√©, recherche g√©n√©rale des URLs");
            download_urls = self.find_all_urls_in_page(document).await?;
        }
        
        info!("üöÄ {} URLs de t√©l√©chargement r√©elles trouv√©es (rapide)", download_urls.len());
        Ok(download_urls)
    }

    /// Trouve toutes les URLs dans une page (m√©thode de fallback)
    async fn find_all_urls_in_page(&self, document: &Html) -> Result<Vec<String>> {
        info!("üîç Recherche g√©n√©rale de toutes les URLs dans la page");
        
        let mut urls = Vec::new();
        
        // Chercher tous les inputs avec des URLs
        let input_selector = Selector::parse("input[type=\"text\"]")
            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les inputs: {}", e))?;
        
        for input in document.select(&input_selector) {
            if let Some(value) = input.value().attr("value") {
                if value.starts_with("http") && !value.contains("t.me") && !value.contains("instagram") && !value.contains("fzmovies.live") {
                    urls.push(value.to_string());
                    info!("üîó URL trouv√©e dans input: {}", value);
                }
            }
        }
        
        // Chercher tous les liens avec des URLs
        let link_selector = Selector::parse("a[href*=\"http\"]")
            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les liens: {}", e))?;
        
        for link in document.select(&link_selector) {
            if let Some(href) = link.value().attr("href") {
                if href.starts_with("http") && !href.contains("t.me") && !href.contains("instagram") && !href.contains("fzmovies.live") {
                    urls.push(href.to_string());
                    info!("üîó URL trouv√©e dans lien: {}", href);
                }
            }
        }
        
        info!("üîç {} URLs trouv√©es dans la page", urls.len());
        Ok(urls)
    }

    /// Test sp√©cifique pour une URL donn√©e avec debug HTML complet et d√©lai de chargement
    pub async fn test_specific_url(&self, url: &str) -> Result<Vec<String>> {
        info!("üß™ Test sp√©cifique pour l'URL: {}", url);
        
        let html = self.fetch_page(url).await?;
        
        // Attendre un peu pour s'assurer que la page est compl√®tement charg√©e
        info!("üß™ Attente de 2 secondes pour le chargement complet de la page...");
        tokio::time::sleep(tokio::time::Duration::from_secs(2)).await;
        
        let document = Html::parse_document(&html);
        
        // Debug HTML complet pour comprendre la structure
        info!("üß™ === DEBUG HTML COMPLET ===");
        info!("üß™ Taille du HTML: {} caract√®res", html.len());
        
        // Chercher tous les divs
        let div_selector = Selector::parse("div").unwrap();
        let mut div_count = 0;
        for div in document.select(&div_selector) {
            div_count += 1;
            if div_count <= 10 { // Limiter √† 10 pour √©viter le spam
                if let Some(class) = div.value().attr("class") {
                    info!("üß™ Div {}: class='{}'", div_count, class);
                } else {
                    info!("üß™ Div {}: pas de class", div_count);
                }
            }
        }
        info!("üß™ Total divs trouv√©s: {}", div_count);
        
        // Chercher tous les inputs
        let input_selector = Selector::parse("input").unwrap();
        let mut input_count = 0;
        for input in document.select(&input_selector) {
            input_count += 1;
            let name = input.value().attr("name").unwrap_or("pas de name");
            let value = input.value().attr("value").unwrap_or("pas de value");
            let input_type = input.value().attr("type").unwrap_or("pas de type");
            info!("üß™ Input {}: name='{}', type='{}', value='{}'", input_count, name, input_type, value);
        }
        info!("üß™ Total inputs trouv√©s: {}", input_count);
        
        // Chercher sp√©cifiquement div.downloadlinks2
        let downloadlinks_selector = Selector::parse("div.downloadlinks2").unwrap();
        let mut downloadlinks_count = 0;
        for div in document.select(&downloadlinks_selector) {
            downloadlinks_count += 1;
            info!("üß™ Div downloadlinks2 {} trouv√©!", downloadlinks_count);
            
            // Chercher les inputs filelink dans ce div
            let filelink_selector = Selector::parse("input[name=\"filelink\"]").unwrap();
            for input in div.select(&filelink_selector) {
                if let Some(value) = input.value().attr("value") {
                    info!("üß™ Input filelink trouv√©: {}", value);
                }
            }
        }
        info!("üß™ Total div.downloadlinks2 trouv√©s: {}", downloadlinks_count);
        
        // Chercher tous les liens
        let link_selector = Selector::parse("a").unwrap();
        let mut link_count = 0;
        for link in document.select(&link_selector) {
            link_count += 1;
            if link_count <= 10 { // Limiter √† 10
                let href = link.value().attr("href").unwrap_or("pas de href");
                let id = link.value().attr("id").unwrap_or("pas d'id");
                info!("üß™ Lien {}: href='{}', id='{}'", link_count, href, id);
            }
        }
        info!("üß™ Total liens trouv√©s: {}", link_count);
        
        info!("üß™ === FIN DEBUG HTML COMPLET ===");
        
        // Utiliser la m√©thode rapide pour extraire les URLs
        let urls = self.scrape_download_page_fast(&document).await?;
        
        info!("üß™ R√©sultat du test: {} URLs trouv√©es", urls.len());
        for (i, url) in urls.iter().enumerate() {
            info!("üß™ URL {}: {}", i + 1, url);
        }
        
        Ok(urls)
    }

    /// Scrape une page download.php pour extraire les URLs de t√©l√©chargement
    async fn scrape_download_page(&self, document: &Html) -> Result<Vec<String>> {
        info!("Recherche des URLs de t√©l√©chargement r√©elles dans la page");
        
        let mut download_urls = Vec::new();
        
        // M√©thode 1: Chercher dans div.downloadlinks2 avec input[name="filelink"]
        let download_links_selector = Selector::parse("div.downloadlinks2")
            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les liens de t√©l√©chargement: {}", e))?;
        
        for element in document.select(&download_links_selector) {
            info!("Div downloadlinks2 trouv√©, recherche des inputs filelink");
            
            // Chercher les inputs avec name="filelink"
            let input_selector = Selector::parse("input[name=\"filelink\"]")
                .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les inputs: {}", e))?;
            
            for input in element.select(&input_selector) {
                if let Some(value) = input.value().attr("value") {
                    download_urls.push(value.to_string());
                    info!("URL de t√©l√©chargement r√©elle trouv√©e: {}", value);
                    
                    // Ouvrir l'URL de t√©l√©chargement finale dans le navigateur pour debug
                    self.open_in_browser(value, "URL de t√©l√©chargement finale");
                }
            }
        }
        
        // M√©thode 2: Si pas trouv√©, chercher directement tous les inputs filelink
        if download_urls.is_empty() {
            info!("Aucun div.downloadlinks2 trouv√©, recherche directe des inputs filelink");
            let input_selector = Selector::parse("input[name=\"filelink\"]")
                .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les inputs: {}", e))?;
            
            for input in document.select(&input_selector) {
                if let Some(value) = input.value().attr("value") {
                    download_urls.push(value.to_string());
                    info!("URL de t√©l√©chargement directe trouv√©e: {}", value);
                }
            }
        }
        
        // M√©thode 3: Chercher aussi dans les liens avec id="flink1", "flink2", etc.
        if download_urls.is_empty() {
            info!("Recherche des liens flink1, flink2, etc.");
            let flink_selector = Selector::parse("a[id^=\"flink\"]")
                .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour les liens flink: {}", e))?;
            
            for link in document.select(&flink_selector) {
                if let Some(href) = link.value().attr("href") {
                    // Construire l'URL compl√®te
                    let full_url = self.resolve_url(href)?;
                    download_urls.push(full_url.clone());
                    info!("Lien flink trouv√©: {}", full_url);
                }
            }
        }
        
        // M√©thode 4: Debug - afficher tous les √©l√©ments qui pourraient contenir des URLs
        if download_urls.is_empty() {
            info!("=== DEBUG: Recherche de tous les √©l√©ments contenant des URLs ===");
            
            let debug_selectors = vec![
                "div[class*=\"download\"]",
                "div[class*=\"link\"]", 
                "input[type=\"text\"]",
                "a[href*=\"filelink\"]",
                "a[href*=\"rlink\"]",
                "a[href*=\"http\"]"
            ];
            
            for selector_str in debug_selectors {
                if let Ok(selector) = Selector::parse(selector_str) {
                    let count = document.select(&selector).count();
                    if count > 0 {
                        info!("S√©lecteur '{}': {} √©l√©ments trouv√©s", selector_str, count);
                        
                        for (i, element) in document.select(&selector).enumerate() {
                            if i >= 2 { break; } // Limiter √† 2 exemples
                            
                            let text = element.text().collect::<String>().trim().to_string();
                            let text_preview = if text.len() > 100 { 
                                format!("{}...", &text[..100]) 
                            } else { 
                                text 
                            };
                            
                            info!("  Exemple {}: {}", i + 1, text_preview);
                            
                            // Afficher les attributs importants
                            if let Some(value) = element.value().attr("value") {
                                info!("    value: {}", value);
                                if value.contains("http") && !download_urls.contains(&value.to_string()) {
                                    download_urls.push(value.to_string());
                                    info!("    -> URL ajout√©e: {}", value);
                                }
                            }
                            if let Some(href) = element.value().attr("href") {
                                info!("    href: {}", href);
                                if href.contains("http") || href.contains("filelink") || href.contains("rlink") {
                                    let full_url = self.resolve_url(href).unwrap_or_else(|_| href.to_string());
                                    if !download_urls.contains(&full_url) {
                                        download_urls.push(full_url.clone());
                                        info!("    -> URL ajout√©e: {}", full_url);
                                    }
                                }
                            }
                        }
                    }
                }
            }
            info!("=== FIN DEBUG ===");
        }
        
        info!("{} URLs de t√©l√©chargement r√©elles trouv√©es", download_urls.len());
        Ok(download_urls)
    }

    /// Scrape une page de t√©l√©chargement depuis une URL
    async fn scrape_download_page_from_url(&self, url: &str) -> Result<Vec<String>> {
        let html = self.fetch_page(url).await?;
        let document = Html::parse_document(&html);
        self.scrape_download_page(&document).await
    }


    /// R√©cup√®re le contenu HTML d'une page
    async fn fetch_page(&self, url: &str) -> Result<String> {
        info!("R√©cup√©ration de la page FZTV: {}", url);
        
        // Acqu√©rir le semaphore pour limiter les requ√™tes concurrentes
        let _permit = self.semaphore
            .acquire()
            .await
            .map_err(|e| anyhow::anyhow!("Erreur d'acquisition du semaphore: {}", e))?;
        
        let response = self.client
            .get(url)
            .send()
            .await
            .context("Erreur lors de la requ√™te HTTP")?;
        
        if !response.status().is_success() {
            return Err(anyhow::anyhow!("Erreur HTTP: {}", response.status()));
        }
        
        let html = response.text().await
            .context("Impossible de lire le contenu de la r√©ponse")?;
        
        Ok(html)
    }

    /// R√©sout une URL relative en URL absolue
    fn resolve_url(&self, url: &str) -> Result<String> {
        if url.starts_with("http://") || url.starts_with("https://") {
            Ok(url.to_string())
        } else {
            let base = Url::parse(&self.base_url)
                .context("URL de base invalide")?;
            let resolved = base.join(url)
                .context("Impossible de r√©soudre l'URL relative")?;
            Ok(resolved.to_string())
        }
    }

    /// Scrape toutes les donn√©es (saisons et √©pisodes) depuis une URL principale
    pub async fn scrape_all(&self, main_url: &str) -> Result<Vec<Season>> {
        info!("D√©but du scraping complet FZTV depuis: {}", main_url);
        
        let seasons = self.scrape_seasons(main_url).await?;
        
        info!("Scraping FZTV termin√©. {} saisons avec un total de {} √©pisodes trouv√©s", 
              seasons.len(), 
              seasons.iter().map(|s| s.episodes.len()).sum::<usize>());
        
        Ok(seasons)
    }

    /// Scrape les liens de t√©l√©chargement r√©els avec traitement rapide pour √©viter l'expiration
    pub async fn scrape_actual_download_link_fast(&self, episode_url: &str) -> Result<Option<String>> {
        info!("üöÄ Scraping rapide du lien de t√©l√©chargement depuis: {}", episode_url);
        
        // Construire l'URL compl√®te
        let full_url = self.resolve_url(episode_url)?;
        
        // R√©cup√©rer le contenu de la page episode.php
        let html = self.fetch_page(&full_url).await?;
        let document = Html::parse_document(&html);
        
        // Chercher le div avec class="mainbox3" et le lien avec id="dlink2"
        let mainbox_selector = Selector::parse("div.mainbox3")
            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour mainbox3: {}", e))?;
        
        let link_selector = Selector::parse("a#dlink2")
            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour dlink2: {}", e))?;
        
        // Chercher dans les divs mainbox3
        for mainbox in document.select(&mainbox_selector) {
            // Chercher le lien dlink2
            for link in mainbox.select(&link_selector) {
                if let Some(onclick) = link.value().attr("onclick") {
                    info!("Onclick trouv√©: {}", onclick);
                    
                    // Extraire l'URL de window.location.href
                    let download_url = if let Some(start) = onclick.find("window.location.href=&quot;") {
                        let start = start + 22; // Longueur de "window.location.href=\""
                        if let Some(end) = onclick[start..].find("\"") {
                            Some(onclick[start..start + end].to_string())
                        } else {
                            None
                        }
                    } else {
                        None
                    };
                    
                    if let Some(url) = download_url {
                        info!("URL de t√©l√©chargement interm√©diaire trouv√©e: {}", url);
                        
                        // Construire l'URL compl√®te en combinant avec la base URL
                        let full_download_url = if url.starts_with("http") {
                            url
                        } else {
                            self.resolve_url(&url)?
                        };
                        
                        info!("URL compl√®te pour le scraping: {}", full_download_url);
                        
                        // Traitement IMM√âDIAT pour √©viter l'expiration
                        let real_urls = self.scrape_download_urls_fast(&full_download_url).await?;
                        if !real_urls.is_empty() {
                            info!("‚úÖ URLs de t√©l√©chargement r√©elles trouv√©es: {:?}", real_urls);
                            return Ok(Some(real_urls[0].clone())); // Retourner la premi√®re URL r√©elle
                        }
                    }
                }
            }
        }
        
        // Si pas trouv√© avec dlink2, essayer avec href direct
        let href_selector = Selector::parse("a[href*=\"downloadmp4.php\"]")
            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour href: {}", e))?;
        
        for link in document.select(&href_selector) {
            if let Some(href) = link.value().attr("href") {
                info!("Href trouv√©: {}", href);
                
                let full_href_url = if href.starts_with("http") {
                    href.to_string()
                } else {
                    self.resolve_url(href)?
                };
                
                info!("URL compl√®te pour le scraping (href): {}", full_href_url);
                
                // Traitement IMM√âDIAT pour √©viter l'expiration
                let real_urls = self.scrape_download_urls_fast(&full_href_url).await?;
                if !real_urls.is_empty() {
                    info!("‚úÖ URLs de t√©l√©chargement r√©elles trouv√©es (href): {:?}", real_urls);
                    return Ok(Some(real_urls[0].clone()));
                }
            }
        }
        
        info!("‚ùå Aucun lien de t√©l√©chargement trouv√© pour: {}", episode_url);
        Ok(None)
    }

    /// Scrape les liens de t√©l√©chargement r√©els depuis une page episode.php
    /// Cette fonction navigue vers la page episode.php, puis vers downloadmp4.php, puis extrait les vraies URLs
    pub async fn scrape_actual_download_link(&self, episode_url: &str) -> Result<Option<String>> {
        info!("Scraping du lien de t√©l√©chargement r√©el depuis: {}", episode_url);
        
        // Construire l'URL compl√®te
        let full_url = self.resolve_url(episode_url)?;
        
        // Ouvrir la page episode.php dans le navigateur pour debug
        self.open_in_browser(&full_url, "Page Episode");
        
        // R√©cup√©rer le contenu de la page episode.php
        let html = self.fetch_page(&full_url).await?;
        let document = Html::parse_document(&html);
        
        // Chercher le div avec class="mainbox3" et le lien avec id="dlink2"
        let mainbox_selector = Selector::parse("div.mainbox3")
            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour mainbox3: {}", e))?;
        
        let link_selector = Selector::parse("a#dlink2")
            .map_err(|e| anyhow::anyhow!("Impossible de cr√©er le s√©lecteur pour dlink2: {}", e))?;
        
        // Chercher dans les divs mainbox3
        for mainbox in document.select(&mainbox_selector) {
            // Chercher le lien dlink2
            for link in mainbox.select(&link_selector) {
                if let Some(onclick) = link.value().attr("onclick") {
                    info!("Onclick trouv√©: {}", onclick);
                    
                    // Extraire l'URL de window.location.href
                    let download_url = if let Some(start) = onclick.find("window.location.href=&quot;") {
                        let start = start + 27; // Longueur de "window.location.href=&quot;"
                        if let Some(end) = onclick[start..].find("&quot;") {
                            Some(onclick[start..start + end].to_string())
                        } else {
                            None
                        }
                    } else if let Some(start) = onclick.find("window.location.href=\"") {
                        let start = start + 22; // Longueur de "window.location.href=\""
                        if let Some(end) = onclick[start..].find("\"") {
                            Some(onclick[start..start + end].to_string())
                        } else {
                            None
                        }
                    } else {
                        None
                    };
                    
                    if let Some(url) = download_url {
                        info!("URL de t√©l√©chargement interm√©diaire trouv√©e: {}", url);
                        
                        // Construire l'URL compl√®te en combinant avec la base URL
                        let full_download_url = if url.starts_with("http") {
                            url
                        } else {
                            self.resolve_url(&url)?
                        };
                        
                        info!("URL compl√®te pour le scraping: {}", full_download_url);
                        
                        // Ouvrir la page de t√©l√©chargement dans le navigateur pour debug
                        self.open_in_browser(&full_download_url, "Page Download");
                        
                        // Maintenant naviguer vers cette page pour obtenir les vraies URLs
                        let real_urls = self.scrape_download_urls(&full_download_url).await?;
                        if !real_urls.is_empty() {
                            info!("URLs de t√©l√©chargement r√©elles trouv√©es: {:?}", real_urls);
                            return Ok(Some(real_urls[0].clone())); // Retourner la premi√®re URL r√©elle
                        }
                    }
                }
                
                // Si pas de onclick, essayer de r√©cup√©rer le href directement
                if let Some(href) = link.value().attr("href") {
                    info!("Href trouv√©: {}", href);
                    if href.contains("downloadmp4.php") {
                        // Construire l'URL compl√®te
                        let full_href_url = if href.starts_with("http") {
                            href.to_string()
                        } else {
                            self.resolve_url(href)?
                        };
                        
                        info!("URL compl√®te pour href: {}", full_href_url);
                        
                        let real_urls = self.scrape_download_urls(&full_href_url).await?;
                        if !real_urls.is_empty() {
                            return Ok(Some(real_urls[0].clone()));
                        }
                    }
                }
            }
        }
        
        info!("Aucun lien de t√©l√©chargement trouv√© dans la page");
        Ok(None)
    }

    /// Enrichit les saisons existantes avec les liens de t√©l√©chargement r√©els
    /// Ne traite que le premier lien "High MP4" ou le premier lien disponible
    pub async fn enrich_with_actual_links(&self, seasons: Vec<Season>) -> Result<Vec<Season>> {
        info!("D√©but de l'enrichissement des liens de t√©l√©chargement");
        
        // Cr√©er une liste de toutes les t√¢ches √† traiter (season_idx, episode_idx, url, quality)
        let mut tasks = Vec::new();
        
        for (season_idx, season) in seasons.iter().enumerate() {
            for (episode_idx, episode) in season.episodes.iter().enumerate() {
                // Trouver l'index du premier lien "High MP4" ou prendre le premier
                let target_index = episode.download_links.iter()
                    .position(|link| link.quality.contains("High MP4"))
                    .or_else(|| {
                        if episode.download_links.is_empty() {
                            None
                        } else {
                            Some(0)
                        }
                    });
                
                if let Some(link_idx) = target_index {
                    let link = &episode.download_links[link_idx];
                    tasks.push((
                        season_idx,
                        episode_idx,
                        link_idx,
                        link.url.clone(),
                        episode.name.clone(),
                    ));
                }
            }
        }
        
        info!("Traitement de {} liens en parall√®le", tasks.len());
        
        // Traiter toutes les t√¢ches en parall√®le avec limitation de concurrence
        let results: Vec<_> = stream::iter(tasks)
            .map(|(season_idx, episode_idx, link_idx, url, episode_name)| async move {
                info!("Scraping du lien pour l'√©pisode: {}", episode_name);
                
                match self.scrape_actual_download_link_fast(&url).await {
                    Ok(Some(download_url)) => {
                        info!("Lien trouv√© pour {}: {}", episode_name, download_url);
                        Some((season_idx, episode_idx, link_idx, download_url))
                    }
                    Ok(None) => {
                        info!("Aucun lien trouv√© pour {}", episode_name);
                        None
                    }
                    Err(e) => {
                        info!("Erreur lors du scraping de {}: {}", episode_name, e);
                        None
                    }
                }
            })
            .buffer_unordered(20)  // Traiter jusqu'√† 20 liens en parall√®le (le semaphore dans fetch_page limite √† 10 requ√™tes r√©elles)
            .filter_map(|x| async { x })
            .collect()
            .await;
        
        // Appliquer les r√©sultats aux saisons
        let mut enriched_seasons = seasons;
        for (season_idx, episode_idx, link_idx, download_url) in results {
            if let Some(season) = enriched_seasons.get_mut(season_idx) {
                if let Some(episode) = season.episodes.get_mut(episode_idx) {
                    if let Some(link) = episode.download_links.get_mut(link_idx) {
                        link.actual_download_urls.push(download_url);
                    }
                }
            }
        }
        
        info!("Enrichissement termin√©");
        Ok(enriched_seasons)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_parse_onclick() {
        let scraper = FztvScraper::new("http://example.com".to_string());
        let onclick = r#"window.open("https://ev.notemandimpled.com/idY6kqCkfWs/kvvRE"); window.location.href="downloadmp4.php?fileid=154326&dkey=d7bf5ed1208135eee507edac13ac6d54"; return false;"#;
        
        let result = scraper.parse_onclick(onclick);
        assert!(result.is_some());
        
        let (url, file_id, dkey) = result.unwrap();
        assert_eq!(url, "downloadmp4.php?fileid=154326&dkey=d7bf5ed1208135eee507edac13ac6d54");
        assert_eq!(file_id, "154326");
        assert_eq!(dkey, Some("d7bf5ed1208135eee507edac13ac6d54".to_string()));
    }
}
